---
title: "Tree-Based Methods For Big (and small) Data"
author: "Miles McBain"
date: "1/18/2017"
output: html_document
---
#Prerequisites
```{r, eval=FALSE}
install.packages(c("rpart.plot"))
```


```{r}
library(rpart)
library(readr)
library(knitr)
library(rpart.plot)

#For your laptop
PRAC_HOME <- here::here()
#For the lab computer
#PRAC_HOME <- "/users/ugrad/amsi_705/MSBigData/Practicals/prac03"
```


#Introduction
In this practical we introduce tree-based approaches with CARTS on a classic data set, then progressing to more advanced tree methods in the Big Data context.

## CARTs

### Surviving the Titanic
The passenger list of those aboard the *RMS Titanic* when disaster struck provides an interesting data set for the application of CARTs to explore relationships between survial and other passenger attributes. 

It's also a great opporunity to check out the analysis of this dataset on [Kaggle](https://www.kaggle.com/c/titanic/details/new-getting-started-with-r).

**Discuss:**

* Based on either historical fact or Hollywood fiction, what are passenger attributes that you would expect to be associated with death or survial? 

###Load Data
```{r, message=FALSE}
titanic_data <- read_csv(file.path(PRAC_HOME, "data/train.csv"))
kable(titanic_data[1:10,])
```

**Discussion**

* To look at passenger survival what kind of model will we fit? 
   - Classification, Regression, Clustering, Dimension Reduction etc?
   - Is it supervised or unsupervised?

### Create a CART
Initially we'll just fit a CART based on `Pclass`, `Sex`, and `Age`.
```{r}
CART_model <- tree_fit_titanic <- rpart(Survived ~ Pclass + Sex + Age,
                          data = titanic_data,
                          method = "class",
                          control = rpart.control(cp = 0.01, xval = 10)
                            )
rpart.plot(tree_fit_titanic)
```

Experiment with the cost-complexity parameter `cp`. In the context of fitting, it controls the size of the tree by setting a threshold of fit improvement that must be met for a node to be added. A lower CP will allow more complex trees to be fit.

Tree models can be pruned to a given cost-complexity after fitting using the `prune()` function.

*Discussion*:

* What do you make of the `xval` parameter?

### CART diagnostics
Try `summary()` but observe that it is not very useful. Plotting the cross-validation error associated with various tree sizes can be used to verify the choice of tree size:
```{r}
plotcp(tree_fit_titanic)
```

We can also make use of `caret::confusionMatrix()` again from the previous practical to check the classification performance.

### Exercise
Experiment with adding other covariates from the dataset and observe their effect on predictive accuracy and how that flows through to tree structure.


### Random Forests & Gradient Boosted Trees
The data for this analysis are real records from a telemarketing campaign run by a Spanish bank. The data contain the records for 32000 calls to banking customers. A bank may collect data like this from a 'pilot' campaign based on customers selected by stratified random sample. The data contain a 0/1 indicator variable which represents the failure/success of the customer accepting the proposed offer.

#### Load Data

#### Labwork
In the lab we can use some R

#### Homework (on your PC)
Fit the Random Forest on H2O.
```{r}
h2o::h2o.randomForest()
```



